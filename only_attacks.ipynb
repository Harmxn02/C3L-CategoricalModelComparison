{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Model Comparison\n",
    "\n",
    "**only_attacks.ipynb**\n",
    "\n",
    "In this notebook we train a model only on attack data, and export the model for use in the **categorical_model_comparison.ipynb** notebook.\n",
    "\n",
    "We will use a CNN-GAN model described in this study: <https://www.jait.us/articles/2024/JAIT-V15N7-886.pdf>\n",
    "\n",
    "Our dataset is the CIC-IDS-2017 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook = \"only_attacks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./data/concat.csv\")\n",
    "\n",
    "# Trim whitespace from column names\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flow Bytes/s                   1358\n",
       "Flow Duration                     0\n",
       "Destination Port                  0\n",
       "Total Backward Packets            0\n",
       "Total Length of Fwd Packets       0\n",
       "                               ... \n",
       "Idle Mean                         0\n",
       "Idle Std                          0\n",
       "Idle Max                          0\n",
       "Idle Min                          0\n",
       "Label                             0\n",
       "Length: 79, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"Flow Bytes/s\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inf. values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace([np.inf, -np.inf], np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Destination Port               0\n",
       "Flow Duration                  0\n",
       "Total Fwd Packets              0\n",
       "Total Backward Packets         0\n",
       "Total Length of Fwd Packets    0\n",
       "                              ..\n",
       "Idle Mean                      0\n",
       "Idle Std                       0\n",
       "Idle Max                       0\n",
       "Idle Min                       0\n",
       "Label                          0\n",
       "Length: 79, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaling numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "numerical_columns = df.select_dtypes(include=\"number\").columns\n",
    "scaler = MinMaxScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map labels to multi-class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**important**: Here we drop all the benign rows, so we are left with only the attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "DoS Hulk                      230124\n",
       "PortScan                      158804\n",
       "DDoS                          128025\n",
       "DoS GoldenEye                  10293\n",
       "FTP-Patator                     7935\n",
       "SSH-Patator                     5897\n",
       "DoS slowloris                   5796\n",
       "DoS Slowhttptest                5499\n",
       "Bot                             1956\n",
       "Web Attack � Brute Force        1507\n",
       "Web Attack � XSS                 652\n",
       "Infiltration                      36\n",
       "Web Attack � Sql Injection        21\n",
       "Heartbleed                        11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"Label\"] != \"BENIGN\"]\n",
    "\n",
    "df[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_mapping = {\n",
    "\t\"DoS Hulk\": 0,\n",
    "\t\"PortScan\": 1,\n",
    "\t\"DDoS\": 2,\n",
    "\t\"DoS GoldenEye\": 3,\n",
    "\t\"FTP-Patator\": 4,\n",
    "\t\"SSH-Patator\": 5,\n",
    "\t\"DoS slowloris\": 6,\n",
    "\t\"DoS Slowhttptest\": 7,\n",
    "\t\"Bot\": 8,\n",
    "\t\"Web Attack � Brute Force\": 9,\n",
    "\t\"Web Attack � XSS\": 10,\n",
    "\t\"Infiltration\": 11,\n",
    "\t\"Web Attack � Sql Injection\": 12,\n",
    "\t\"Heartbleed\": 13\n",
    "}\n",
    "\n",
    "df[\"Label\"] = df[\"Label\"].map(attack_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0     230124\n",
       "1     158804\n",
       "2     128025\n",
       "3      10293\n",
       "4       7935\n",
       "5       5897\n",
       "6       5796\n",
       "7       5499\n",
       "8       1956\n",
       "9       1507\n",
       "10       652\n",
       "11        36\n",
       "12        21\n",
       "13        11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Label\"])\n",
    "y = df[\"Label\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overlap between sets\n",
    "\n",
    "train_hashes = set(X_train.index)\n",
    "test_hashes = set(X_test.index)\n",
    "\n",
    "assert train_hashes.isdisjoint(test_hashes), \"Overlap detected between training and test sets!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate samples: 0\n"
     ]
    }
   ],
   "source": [
    "overlap = set(map(tuple, X_train.values)) & set(map(tuple, X_test.values))\n",
    "print(f\"Number of duplicate samples: {len(overlap)}\")\n",
    "assert len(overlap) == 0, \"Data leakage detected!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample all values below 10.000\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy={\n",
    "\t0: 1000,\n",
    "\t1: 1000,\n",
    "\t2: 1000, \n",
    "\t3: 1000,\n",
    "\t4: 1000,\n",
    "\t5: 1000,\n",
    "\t6: 1000,\n",
    "\t7: 1000,\n",
    "\t8: 1000,\n",
    "\t9: 1000,\n",
    "}, random_state=28)\n",
    "\n",
    "rus_testset = RandomUnderSampler(sampling_strategy={\n",
    "\t0: 200,\n",
    "\t1: 200,\n",
    "\t2: 200, \n",
    "\t3: 200,\n",
    "\t4: 200,\n",
    "\t5: 200,\n",
    "\t6: 200,\n",
    "\t7: 200,\n",
    "\t8: 200,\n",
    "\t9: 200,\n",
    "}, random_state=28)\n",
    "\n",
    "X_train_balanced, y_train_balanced = rus.fit_resample(X_train, y_train)\n",
    "X_test_balanced, y_test_balanced = rus_testset.fit_resample(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample all values below 10.000\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy={\n",
    "\t10: 1000,\n",
    "\t11: 1000,\n",
    "\t12: 1000,\n",
    "\t13: 1000,\n",
    "}, random_state=28)\n",
    "\n",
    "ros_testset = RandomOverSampler(sampling_strategy={\n",
    "\t10: 200,\n",
    "\t11: 200,\n",
    "\t12: 200,\n",
    "\t13: 200\n",
    "}, random_state=28)\n",
    "\n",
    "X_train_balanced, y_train_balanced = ros.fit_resample(X_train_balanced, y_train_balanced)\n",
    "X_test_balanced, y_test_balanced = ros_testset.fit_resample(X_test_balanced, y_test_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE: Counter({0: 138276, 2: 102411, 1: 72555, 3: 8229, 4: 4745, 6: 4308, 7: 4182, 5: 2575, 8: 1558, 9: 1176, 10: 522, 11: 29, 12: 17, 13: 9})\n",
      "Class distribution after SMOTE: Counter({0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000, 10: 1000, 11: 1000, 12: 1000, 13: 1000})\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution after SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "print(f\"Class distribution before SMOTE: {Counter(y_train)}\")\n",
    "print(f\"Class distribution after SMOTE: {Counter(y_train_balanced)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CNN Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Define CNN Feature Extractor\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_size, num_filters=32):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear((input_size // 2) * num_filters, 64)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generator-Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Define Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hybrid Model\n",
    "class HybridCNNGAN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, noise_dim=32):\n",
    "        super(HybridCNNGAN, self).__init__()\n",
    "        self.feature_extractor = CNNFeatureExtractor(input_size)\n",
    "        self.classifier = nn.Linear(64, output_size)\n",
    "        self.generator = Generator(noise_dim, input_size)\n",
    "        self.discriminator = Discriminator(input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return self.classifier(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Model Summary\n",
      "------------------------------\n",
      "HybridCNNGAN(\n",
      "  (feature_extractor): CNNFeatureExtractor(\n",
      "    (conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (relu): ReLU()\n",
      "    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=1248, out_features=64, bias=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=64, out_features=14, bias=True)\n",
      "  (generator): Generator(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=78, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (discriminator): Discriminator(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=78, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "------------------------------\n",
      "Device: cuda\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "input_size = X_train_balanced.shape[1]\n",
    "output_size = len(attack_mapping)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HybridCNNGAN(input_size, output_size).to(device)\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Model Summary\")\n",
    "print(\"-\"*30)\n",
    "print(model)\n",
    "print(\"-\"*30)\n",
    "print(\"Device:\", device)\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] - Loss: 1.5666 - Accuracy: 0.5243\n",
      "Epoch [2/1000] - Loss: 0.7968 - Accuracy: 0.7255\n",
      "Epoch [3/1000] - Loss: 0.6422 - Accuracy: 0.7686\n",
      "Epoch [4/1000] - Loss: 0.5598 - Accuracy: 0.7976\n",
      "Epoch [5/1000] - Loss: 0.4955 - Accuracy: 0.8185\n",
      "Epoch [6/1000] - Loss: 0.4466 - Accuracy: 0.8450\n",
      "Epoch [7/1000] - Loss: 0.4118 - Accuracy: 0.8595\n",
      "Epoch [8/1000] - Loss: 0.3742 - Accuracy: 0.8726\n",
      "Epoch [9/1000] - Loss: 0.3539 - Accuracy: 0.8794\n",
      "Epoch [10/1000] - Loss: 0.3371 - Accuracy: 0.8807\n",
      "Epoch [11/1000] - Loss: 0.3210 - Accuracy: 0.8823\n",
      "Epoch [12/1000] - Loss: 0.3087 - Accuracy: 0.8858\n",
      "Epoch [13/1000] - Loss: 0.2982 - Accuracy: 0.8861\n",
      "Epoch [14/1000] - Loss: 0.2944 - Accuracy: 0.8875\n",
      "Epoch [15/1000] - Loss: 0.2827 - Accuracy: 0.8882\n",
      "Epoch [16/1000] - Loss: 0.2778 - Accuracy: 0.8886\n",
      "Epoch [17/1000] - Loss: 0.2664 - Accuracy: 0.8946\n",
      "Epoch [18/1000] - Loss: 0.2576 - Accuracy: 0.8976\n",
      "Epoch [19/1000] - Loss: 0.2556 - Accuracy: 0.8949\n",
      "Epoch [20/1000] - Loss: 0.2486 - Accuracy: 0.8968\n",
      "Epoch [21/1000] - Loss: 0.2458 - Accuracy: 0.8981\n",
      "Epoch [22/1000] - Loss: 0.2372 - Accuracy: 0.8984\n",
      "Epoch [24/1000] - Loss: 0.2341 - Accuracy: 0.9020\n",
      "Epoch [26/1000] - Loss: 0.2238 - Accuracy: 0.9031\n",
      "Epoch [28/1000] - Loss: 0.2230 - Accuracy: 0.9024\n",
      "Epoch [29/1000] - Loss: 0.2149 - Accuracy: 0.9047\n",
      "Epoch [31/1000] - Loss: 0.2131 - Accuracy: 0.9048\n",
      "Epoch [32/1000] - Loss: 0.2102 - Accuracy: 0.9072\n",
      "Epoch [34/1000] - Loss: 0.2061 - Accuracy: 0.9072\n",
      "Epoch [35/1000] - Loss: 0.2044 - Accuracy: 0.9057\n",
      "Epoch [36/1000] - Loss: 0.2026 - Accuracy: 0.9086\n",
      "Epoch [37/1000] - Loss: 0.1973 - Accuracy: 0.9088\n",
      "Epoch [38/1000] - Loss: 0.1942 - Accuracy: 0.9107\n",
      "Epoch [39/1000] - Loss: 0.1935 - Accuracy: 0.9126\n",
      "Epoch [44/1000] - Loss: 0.1900 - Accuracy: 0.9127\n",
      "Epoch [45/1000] - Loss: 0.1893 - Accuracy: 0.9110\n",
      "Epoch [47/1000] - Loss: 0.1873 - Accuracy: 0.9134\n",
      "Epoch [49/1000] - Loss: 0.1846 - Accuracy: 0.9123\n",
      "Epoch [51/1000] - Loss: 0.1832 - Accuracy: 0.9143\n",
      "Epoch [54/1000] - Loss: 0.1777 - Accuracy: 0.9161\n",
      "Epoch [55/1000] - Loss: 0.1770 - Accuracy: 0.9158\n",
      "Epoch [58/1000] - Loss: 0.1734 - Accuracy: 0.9220\n",
      "Epoch [69/1000] - Loss: 0.1729 - Accuracy: 0.9181\n",
      "Epoch [70/1000] - Loss: 0.1723 - Accuracy: 0.9174\n",
      "Epoch [71/1000] - Loss: 0.1683 - Accuracy: 0.9204\n",
      "Epoch [74/1000] - Loss: 0.1683 - Accuracy: 0.9212\n",
      "Epoch [80/1000] - Loss: 0.1665 - Accuracy: 0.9209\n",
      "Epoch [84/1000] - Loss: 0.1650 - Accuracy: 0.9201\n",
      "Epoch [85/1000] - Loss: 0.1646 - Accuracy: 0.9185\n",
      "Epoch [89/1000] - Loss: 0.1630 - Accuracy: 0.9221\n",
      "Epoch [95/1000] - Loss: 0.1609 - Accuracy: 0.9231\n",
      "Epoch [99/1000] - Loss: 0.1606 - Accuracy: 0.9226\n",
      "Epoch [100/1000] - Loss: 0.1598 - Accuracy: 0.9214\n",
      "Epoch [104/1000] - Loss: 0.1594 - Accuracy: 0.9230\n",
      "Epoch [106/1000] - Loss: 0.1594 - Accuracy: 0.9224\n",
      "Epoch [107/1000] - Loss: 0.1587 - Accuracy: 0.9238\n",
      "Epoch [108/1000] - Loss: 0.1577 - Accuracy: 0.9221\n",
      "Epoch [115/1000] - Loss: 0.1559 - Accuracy: 0.9231\n",
      "Epoch [118/1000] - Loss: 0.1542 - Accuracy: 0.9246\n",
      "Epoch [125/1000] - Loss: 0.1541 - Accuracy: 0.9256\n",
      "Epoch [127/1000] - Loss: 0.1538 - Accuracy: 0.9256\n",
      "Epoch [129/1000] - Loss: 0.1506 - Accuracy: 0.9251\n",
      "Epoch [141/1000] - Loss: 0.1494 - Accuracy: 0.9248\n",
      "Epoch [152/1000] - Loss: 0.1491 - Accuracy: 0.9270\n",
      "Epoch [153/1000] - Loss: 0.1480 - Accuracy: 0.9274\n",
      "Epoch [160/1000] - Loss: 0.1455 - Accuracy: 0.9274\n",
      "Epoch [169/1000] - Loss: 0.1444 - Accuracy: 0.9291\n",
      "Epoch [180/1000] - Loss: 0.1440 - Accuracy: 0.9294\n",
      "Epoch [183/1000] - Loss: 0.1437 - Accuracy: 0.9294\n",
      "Epoch [186/1000] - Loss: 0.1429 - Accuracy: 0.9286\n",
      "Epoch [192/1000] - Loss: 0.1422 - Accuracy: 0.9284\n",
      "Epoch [201/1000] - Loss: 0.1403 - Accuracy: 0.9296\n",
      "Epoch [221/1000] - Loss: 0.1403 - Accuracy: 0.9288\n",
      "Epoch [224/1000] - Loss: 0.1385 - Accuracy: 0.9314\n",
      "Epoch [239/1000] - Loss: 0.1381 - Accuracy: 0.9307\n",
      "Epoch [246/1000] - Loss: 0.1380 - Accuracy: 0.9306\n",
      "Epoch [248/1000] - Loss: 0.1371 - Accuracy: 0.9314\n",
      "Epoch [261/1000] - Loss: 0.1366 - Accuracy: 0.9299\n",
      "Epoch [262/1000] - Loss: 0.1356 - Accuracy: 0.9337\n",
      "Epoch [265/1000] - Loss: 0.1353 - Accuracy: 0.9351\n",
      "Epoch [282/1000] - Loss: 0.1343 - Accuracy: 0.9332\n",
      "Epoch [303/1000] - Loss: 0.1342 - Accuracy: 0.9299\n",
      "Epoch [312/1000] - Loss: 0.1341 - Accuracy: 0.9322\n",
      "Epoch [314/1000] - Loss: 0.1340 - Accuracy: 0.9345\n",
      "Epoch [316/1000] - Loss: 0.1327 - Accuracy: 0.9319\n",
      "Epoch [325/1000] - Loss: 0.1321 - Accuracy: 0.9354\n",
      "Epoch [345/1000] - Loss: 0.1314 - Accuracy: 0.9351\n",
      "Epoch [348/1000] - Loss: 0.1313 - Accuracy: 0.9345\n",
      "Epoch [365/1000] - Loss: 0.1309 - Accuracy: 0.9349\n",
      "Epoch [374/1000] - Loss: 0.1306 - Accuracy: 0.9351\n",
      "Epoch [380/1000] - Loss: 0.1301 - Accuracy: 0.9351\n",
      "Epoch [405/1000] - Loss: 0.1297 - Accuracy: 0.9363\n",
      "Epoch [408/1000] - Loss: 0.1293 - Accuracy: 0.9335\n",
      "Epoch [416/1000] - Loss: 0.1291 - Accuracy: 0.9367\n",
      "Epoch [419/1000] - Loss: 0.1291 - Accuracy: 0.9359\n",
      "Epoch [420/1000] - Loss: 0.1291 - Accuracy: 0.9354\n",
      "Epoch [421/1000] - Loss: 0.1289 - Accuracy: 0.9348\n",
      "Epoch [433/1000] - Loss: 0.1287 - Accuracy: 0.9359\n",
      "Epoch [466/1000] - Loss: 0.1279 - Accuracy: 0.9344\n",
      "Epoch [487/1000] - Loss: 0.1278 - Accuracy: 0.9369\n",
      "Epoch [489/1000] - Loss: 0.1277 - Accuracy: 0.9349\n",
      "Epoch [492/1000] - Loss: 0.1269 - Accuracy: 0.9361\n",
      "Epoch [512/1000] - Loss: 0.1267 - Accuracy: 0.9351\n",
      "Epoch [515/1000] - Loss: 0.1262 - Accuracy: 0.9371\n",
      "Epoch [533/1000] - Loss: 0.1257 - Accuracy: 0.9374\n",
      "Epoch [539/1000] - Loss: 0.1255 - Accuracy: 0.9391\n",
      "Epoch [550/1000] - Loss: 0.1250 - Accuracy: 0.9358\n",
      "Epoch [557/1000] - Loss: 0.1249 - Accuracy: 0.9357\n",
      "Epoch [571/1000] - Loss: 0.1248 - Accuracy: 0.9381\n",
      "Epoch [577/1000] - Loss: 0.1242 - Accuracy: 0.9366\n",
      "Epoch [588/1000] - Loss: 0.1240 - Accuracy: 0.9362\n",
      "Epoch [589/1000] - Loss: 0.1233 - Accuracy: 0.9359\n",
      "Epoch [629/1000] - Loss: 0.1223 - Accuracy: 0.9362\n",
      "Epoch [663/1000] - Loss: 0.1222 - Accuracy: 0.9390\n",
      "Epoch [669/1000] - Loss: 0.1218 - Accuracy: 0.9396\n",
      "Epoch [672/1000] - Loss: 0.1213 - Accuracy: 0.9369\n",
      "Epoch [681/1000] - Loss: 0.1206 - Accuracy: 0.9400\n",
      "Epoch [728/1000] - Loss: 0.1201 - Accuracy: 0.9405\n",
      "Epoch [752/1000] - Loss: 0.1199 - Accuracy: 0.9397\n",
      "Epoch [767/1000] - Loss: 0.1186 - Accuracy: 0.9405\n",
      "Epoch [780/1000] - Loss: 0.1179 - Accuracy: 0.9390\n",
      "Epoch [801/1000] - Loss: 0.1179 - Accuracy: 0.9389\n",
      "Epoch [823/1000] - Loss: 0.1178 - Accuracy: 0.9409\n",
      "Epoch [831/1000] - Loss: 0.1177 - Accuracy: 0.9391\n",
      "Epoch [832/1000] - Loss: 0.1172 - Accuracy: 0.9399\n",
      "Epoch [840/1000] - Loss: 0.1162 - Accuracy: 0.9401\n",
      "Epoch [862/1000] - Loss: 0.1157 - Accuracy: 0.9399\n",
      "Epoch [883/1000] - Loss: 0.1156 - Accuracy: 0.9389\n",
      "Epoch [889/1000] - Loss: 0.1152 - Accuracy: 0.9389\n",
      "Epoch [904/1000] - Loss: 0.1152 - Accuracy: 0.9392\n",
      "Epoch [906/1000] - Loss: 0.1148 - Accuracy: 0.9404\n",
      "Epoch [916/1000] - Loss: 0.1145 - Accuracy: 0.9421\n",
      "Epoch [930/1000] - Loss: 0.1144 - Accuracy: 0.9390\n",
      "Epoch [941/1000] - Loss: 0.1140 - Accuracy: 0.9401\n",
      "Epoch [946/1000] - Loss: 0.1136 - Accuracy: 0.9407\n",
      "Epoch [952/1000] - Loss: 0.1136 - Accuracy: 0.9409\n",
      "Epoch [958/1000] - Loss: 0.1130 - Accuracy: 0.9407\n",
      "Epoch [989/1000] - Loss: 0.1127 - Accuracy: 0.9421\n",
      "Epoch [993/1000] - Loss: 0.1126 - Accuracy: 0.9401\n",
      "Epoch [1000/1000] - Loss: 0.1124 - Accuracy: 0.9413\n"
     ]
    }
   ],
   "source": [
    "# Early stopping setup\n",
    "early_stopping_patience = 50\n",
    "best_loss = float(\"inf\")\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training Setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_balanced.values, dtype=torch.float32).to(device),\n",
    "                              torch.tensor(y_train_balanced.values, dtype=torch.long).to(device))\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "\tmodel.train()\n",
    "\ttotal_loss, correct, total = 0, 0, 0\n",
    "\tfor i, (data, labels) in enumerate(train_loader):\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = model(data)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\t\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\n",
    "\t\ttotal_loss += loss.item()\n",
    "\t\t_, predicted = torch.max(outputs.data, 1)\n",
    "\t\ttotal += labels.size(0)\n",
    "\t\t\n",
    "\t\tcorrect += (predicted == labels).sum().item()\n",
    "\t\tprogress = (i + 1) / len(train_loader) * 100\n",
    "\t\t\n",
    "\t\t# print(f'\\rEpoch [{epoch+1}/{num_epochs}] - Progress: {progress:.1f}%', end='')\n",
    "\n",
    "\tepoch_loss = total_loss / len(train_loader)\n",
    "\tepoch_accuracy = correct / total\n",
    "\n",
    "\t# print(f' - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "\t# Early Stopping Condition\n",
    "\tif epoch_loss < best_loss:\n",
    "\t\tbest_loss = epoch_loss\n",
    "\t\tepochs_without_improvement = 0\n",
    "\t\tprint(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.4f}\")\n",
    "\telse:\n",
    "\t\tepochs_without_improvement += 1\n",
    "\t\tprint(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.4f}\", end=\"\\r\")\n",
    "\t\n",
    "\tif epochs_without_improvement >= early_stopping_patience:\n",
    "\t\tprint(f\"Early stopping triggered at epoch {epoch+1} due to no improvement.\")\n",
    "\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"./models/{notebook}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9414\n",
      "F1 Score: 0.9391\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                  DoS Hulk       1.00      0.99      0.99       200\n",
      "                  PortScan       1.00      1.00      1.00       200\n",
      "                      DDoS       0.99      0.99      0.99       200\n",
      "             DoS GoldenEye       0.99      0.98      0.99       200\n",
      "               FTP-Patator       0.99      0.99      0.99       200\n",
      "               SSH-Patator       0.99      0.99      0.99       200\n",
      "             DoS slowloris       0.99      0.99      0.99       200\n",
      "          DoS Slowhttptest       0.88      0.99      0.93       200\n",
      "                       Bot       1.00      1.00      1.00       200\n",
      "  Web Attack � Brute Force       0.82      0.48      0.61       200\n",
      "          Web Attack � XSS       0.65      0.88      0.74       200\n",
      "              Infiltration       1.00      0.88      0.93       200\n",
      "Web Attack � Sql Injection       0.94      1.00      0.97       200\n",
      "                Heartbleed       1.00      1.00      1.00       200\n",
      "\n",
      "                  accuracy                           0.94      2800\n",
      "                 macro avg       0.95      0.94      0.94      2800\n",
      "              weighted avg       0.95      0.94      0.94      2800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_balanced.values, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_balanced.values, dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "\toutputs = model(X_test_tensor)\n",
    "\t_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test_tensor.cpu(), predicted.cpu()):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test_tensor.cpu(), predicted.cpu(), average=\"weighted\"):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test_tensor.cpu(), predicted.cpu(), target_names=attack_mapping))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
